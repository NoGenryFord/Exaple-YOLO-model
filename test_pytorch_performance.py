"""
–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó PyTorch YOLO –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ TFLite –≤–µ—Ä—Å—ñ—î—é
"""

from ultralytics import YOLO
import cv2 as cv
import numpy as np
import time
import os

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
CONFIDENCE_THRESHOLD = 0.3
VIDEO_FILES = [
    "data/sample_battle_1.mp4",
    "data/sample_battle_2.mp4", 
    "data/sample_battle_3.MP4",
    "data/tank1.mp4",
    "data/tank2.mp4"
]

def test_pytorch_vs_tflite():
    """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ PyTorch —Ç–∞ TFLite"""
    
    print("=== –¢–µ—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ PyTorch YOLO ===")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ PyTorch –º–æ–¥–µ–ª—å
    try:
        model = YOLO("weights/YOLO/model_3_best.pt")
        print("‚úÖ PyTorch –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è PyTorch –º–æ–¥–µ–ª—ñ: {e}")
        return
    
    # –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—ñ–¥–µ–æ
    video_path = VIDEO_FILES[0] if os.path.exists(VIDEO_FILES[0]) else None
    if video_path:
        cap = cv.VideoCapture(video_path)
        print(f"‚úÖ –í—ñ–¥–µ–æ –≤—ñ–¥–∫—Ä–∏—Ç–æ: {video_path}")
    else:
        cap = cv.VideoCapture(0)
        print("‚úÖ –ö–∞–º–µ—Ä–∞ –≤—ñ–¥–∫—Ä–∏—Ç–∞")
    
    if not cap.isOpened():
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –¥–∂–µ—Ä–µ–ª–æ –≤—ñ–¥–µ–æ")
        return
    
    # –¢–µ—Å—Ç—É—î–º–æ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å
    frame_count = 0
    detection_times = []
    total_detections = 0
    
    print("üîÑ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ...")
    
    while frame_count < 100:  # –¢–µ—Å—Ç—É—î–º–æ 100 –∫–∞–¥—Ä—ñ–≤
        ret, frame = cap.read()
        if not ret:
            break
        
        # –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–∞–¥—Ä—É
        frame = cv.resize(frame, (640, 480))
        
        # –¢–µ—Å—Ç—É—î–º–æ –¥–µ—Ç–µ–∫—Ü—ñ—é –∫–æ–∂–µ–Ω 3-–π –∫–∞–¥—Ä
        if frame_count % 3 == 0:
            start_time = time.time()
            
            # PyTorch –¥–µ—Ç–µ–∫—Ü—ñ—è
            results = model(frame, conf=CONFIDENCE_THRESHOLD)
            
            detection_time = time.time() - start_time
            detection_times.append(detection_time)
            
            # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ –¥–µ—Ç–µ–∫—Ü—ñ—ó
            if results and len(results) > 0:
                boxes = results[0].boxes
                if boxes is not None:
                    total_detections += len(boxes)
        
        frame_count += 1
        
        # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
        if frame_count % 10 == 0:
            print(f"   –û–±—Ä–æ–±–ª–µ–Ω–æ {frame_count}/100 –∫–∞–¥—Ä—ñ–≤...")
    
    cap.release()
    
    # –ü–æ–∫–∞–∑—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    if detection_times:
        avg_detection_time = np.mean(detection_times)
        max_detection_time = max(detection_times)
        min_detection_time = min(detection_times)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ PyTorch YOLO:")
        print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –¥–µ—Ç–µ–∫—Ü—ñ—ó: {avg_detection_time:.3f}s")
        print(f"   –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —á–∞—Å: {min_detection_time:.3f}s")
        print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —á–∞—Å: {max_detection_time:.3f}s")
        print(f"   –¢–µ–æ—Ä–µ—Ç–∏—á–Ω–∏–π FPS: {1.0/avg_detection_time:.1f}")
        print(f"   –í—Å—å–æ–≥–æ –¥–µ—Ç–µ–∫—Ü—ñ–π: {total_detections}")
        print(f"   –°–µ—Ä–µ–¥–Ω—å–æ –¥–µ—Ç–µ–∫—Ü—ñ–π –Ω–∞ –∫–∞–¥—Ä: {total_detections/len(detection_times):.1f}")
        
        # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ TFLite
        print(f"\nüîÑ –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è:")
        print(f"   PyTorch: {1.0/avg_detection_time:.1f} FPS")
        print(f"   TFLite (–æ—á—ñ–∫—É—î—Ç—å—Å—è): ~30-60 FPS")
        
        if avg_detection_time > 0.033:  # 30 FPS
            print("   ‚ö†Ô∏è  PyTorch –º–æ–¥–µ–ª—å –ø–æ–≤—ñ–ª—å–Ω–∞ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —á–∞—Å—É")
        else:
            print("   ‚úÖ PyTorch –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —à–≤–∏–¥–∫–∞")

def run_pytorch_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è PyTorch –º–æ–¥–µ–ª—ñ"""
    
    print("\n=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è PyTorch YOLO ===")
    print("–ù–∞—Ç–∏—Å–Ω—ñ—Ç—å ESC –¥–ª—è –≤–∏—Ö–æ–¥—É, 'v' –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è –≤—ñ–¥–µ–æ")
    
    try:
        model = YOLO("weights/YOLO/model_3_best.pt")
        print("‚úÖ PyTorch –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è PyTorch –º–æ–¥–µ–ª—ñ: {e}")
        return
    
    # –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ –≤—ñ–¥–µ–æ
    current_video = 0
    cap = cv.VideoCapture(VIDEO_FILES[current_video] if os.path.exists(VIDEO_FILES[current_video]) else 0)
    
    fps_counter = 0
    fps_start_time = time.time()
    fps_display = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            # –ü–µ—Ä–µ–∫–ª—é—á–∞—î–º–æ—Å—è –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–µ –≤—ñ–¥–µ–æ
            current_video = (current_video + 1) % len(VIDEO_FILES)
            cap.release()
            cap = cv.VideoCapture(VIDEO_FILES[current_video])
            continue
        
        # –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–∞–¥—Ä—É
        frame = cv.resize(frame, (640, 480))
        
        # –î–µ—Ç–µ–∫—Ü—ñ—è
        results = model(frame, conf=CONFIDENCE_THRESHOLD)
        
        # –ú–∞–ª—é—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        if results and len(results) > 0:
            annotated_frame = results[0].plot()
        else:
            annotated_frame = frame
        
        # –†–∞—Ö—É—î–º–æ FPS
        fps_counter += 1
        if time.time() - fps_start_time >= 1.0:
            fps_display = fps_counter
            fps_counter = 0
            fps_start_time = time.time()
        
        # –î–æ–¥–∞—î–º–æ FPS –¥–æ –∫–∞–¥—Ä—É
        cv.putText(annotated_frame, f"PyTorch FPS: {fps_display}", 
                  (10, 30), cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        cv.imshow("PyTorch YOLO Demo", annotated_frame)
        
        # –û–±—Ä–æ–±–∫–∞ –∫–ª–∞–≤—ñ—à
        key = cv.waitKey(1) & 0xFF
        if key == 27:  # ESC
            break
        elif key == ord('v'):
            current_video = (current_video + 1) % len(VIDEO_FILES)
            cap.release()
            cap = cv.VideoCapture(VIDEO_FILES[current_video])
    
    cap.release()
    cv.destroyAllWindows()

if __name__ == "__main__":
    test_pytorch_vs_tflite()
    
    # –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞—î–º–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—é
    response = input("\nüé¨ –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—é PyTorch? (y/n): ")
    if response.lower() == 'y':
        run_pytorch_demo()
