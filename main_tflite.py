"""
TensorFlow Lite –≤–µ—Ä—Å—ñ—è main.py –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –∫–æ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–æ—é YOLO –º–æ–¥–µ–ª–ª—é
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –¥–ª—è Raspberry Pi —Ç–∞ —ñ–Ω—à–∏—Ö ARM –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤
"""

import tensorflow as tf
from deep_sort_realtime.deepsort_tracker import DeepSort
import cv2 as cv
import numpy as np
import time
import os

# ----------------------
# OpenCV performance settings
# ----------------------
cv.setUseOptimized(True)
cv.setNumThreads(4)

# ----------------------
# Constants
# ----------------------
STANDARD_WIDTH = 640  # Standard frame width
STANDARD_HEIGHT = 480  # Standard frame height
MAX_FPS = 60  # –ó–±—ñ–ª—å—à—É—î–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π FPS
YOLO_SKIP_FRAMES = 2  # –ó–º–µ–Ω—à—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∫–∞–¥—Ä—ñ–≤ –¥–ª—è –∫—Ä–∞—â–æ—ó –¥–µ—Ç–µ–∫—Ü—ñ—ó

# Model paths
TFLITE_MODEL_PATH = "weights/YOLO/model_3_simple.tflite"
CONFIDENCE_THRESHOLD = 0.5  # –ü—ñ–¥–≤–∏—â—É—î–º–æ –ø–æ—Ä—ñ–≥ –¥–ª—è –±—ñ–ª—å—à —Ç–æ—á–Ω–æ—ó –¥–µ—Ç–µ–∫—Ü—ñ—ó
IOU_THRESHOLD = 0.4

# Video paths
VIDEO_FILES = [
    "data/sample_battle_1.mp4",
    "data/sample_battle_2.mp4", 
    "data/sample_battle_3.MP4",
    "data/tank1.mp4",
    "data/tank2.mp4"
]

# ----------------------
# TFLite Model Class
# ----------------------
class TFLiteYOLO:
    def __init__(self, model_path):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è TFLite YOLO –º–æ–¥–µ–ª—ñ"""
        self.model_path = model_path
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        self.input_shape = None
        
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î TFLite –º–æ–¥–µ–ª—å"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"TFLite –º–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞: {self.model_path}")
        
        print(f"üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ TFLite –º–æ–¥–µ–ª—å: {self.model_path}")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ç–æ—Ä
        self.interpreter = tf.lite.Interpreter(model_path=self.model_path)
        self.interpreter.allocate_tensors()
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –¥–µ—Ç–∞–ª—ñ –≤—Ö—ñ–¥–Ω–∏—Ö —Ç–∞ –≤–∏—Ö—ñ–¥–Ω–∏—Ö —Ç–µ–Ω–∑–æ—Ä—ñ–≤
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        self.input_shape = self.input_details[0]['shape']
        print(f"üìä –í—Ö—ñ–¥–Ω–∞ —Ñ–æ—Ä–º–∞: {self.input_shape}")
        print(f"üìä –í–∏—Ö—ñ–¥–Ω–∞ —Ñ–æ—Ä–º–∞: {self.output_details[0]['shape']}")
        
        print("‚úÖ TFLite –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ!")
    
    def preprocess_frame(self, frame):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–∞–¥—Ä—É –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É"""
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ
        input_shape = self.input_shape
        
        # –†–æ–∑–±–∏—Ä–∞—î–º–æ —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–∞
        if input_shape[1] == 3:  # NCHW format: [batch, channels, height, width]
            model_height = input_shape[2]
            model_width = input_shape[3]
        else:  # NHWC format: [batch, height, width, channels]
            model_height = input_shape[1]
            model_width = input_shape[2]
        
        # –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–∞–¥—Ä—É
        resized = cv.resize(frame, (model_width, model_height))
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ BGR –≤ RGB
        rgb_frame = cv.cvtColor(resized, cv.COLOR_BGR2RGB)
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ–∫—Å–µ–ª—ñ–≤ [0-255] -> [0-1]
        normalized = rgb_frame.astype(np.float32) / 255.0
        
        # –ì–æ—Ç—É—î–º–æ —Ç–µ–Ω–∑–æ—Ä –∑–≥—ñ–¥–Ω–æ –∑ —Ñ–æ—Ä–º–∞—Ç–æ–º –º–æ–¥–µ–ª—ñ
        if input_shape[1] == 3:  # NCHW format
            # –¢—Ä–∞–Ω—Å–ø–æ–Ω—É—î–º–æ –≤ NCHW: [H, W, C] -> [C, H, W]
            normalized = np.transpose(normalized, (2, 0, 1))
            # –î–æ–¥–∞—î–º–æ batch dimension: [C, H, W] -> [1, C, H, W]
            input_data = np.expand_dims(normalized, axis=0)
        else:  # NHWC format
            # –î–æ–¥–∞—î–º–æ batch dimension: [H, W, C] -> [1, H, W, C]
            input_data = np.expand_dims(normalized, axis=0)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ñ–æ—Ä–º—É (–¥–ª—è debug - –º–æ–∂–Ω–∞ –≤–∏–¥–∞–ª–∏—Ç–∏)
        # print(f"üîß –í—Ö—ñ–¥–Ω–∞ —Ñ–æ—Ä–º–∞ –ø—ñ—Å–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥—É: {input_data.shape}")
        # print(f"üîß –û—á—ñ–∫—É–≤–∞–Ω–∞ —Ñ–æ—Ä–º–∞: {input_shape}")
        
        return input_data.astype(np.float32)
    
    def postprocess_output(self, output_data, original_shape):
        """–û–±—Ä–æ–±–∫–∞ –≤–∏—Ö–æ–¥—É TFLite YOLO –º–æ–¥–µ–ª—ñ"""
        if output_data is None:
            return []
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–æ–∑–º—ñ—Ä–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä—É
        orig_height, orig_width = original_shape[:2]
        
        # –î–ª—è —Ü—ñ—î—ó TFLite –º–æ–¥–µ–ª—ñ –≤–∏—Ö—ñ–¥ –º–∞—î —Ñ–æ—Ä–º—É [1, 5, 8400]
        # 5 = [x_center, y_center, width, height, confidence]
        predictions = output_data[0]  # –í–∏–¥–∞–ª—è—î–º–æ batch dimension -> [5, 8400]
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω—É—î–º–æ –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ: [5, 8400] -> [8400, 5]
        predictions = predictions.T
        
        detections = []
        
        for pred in predictions:
            # –û—Ç—Ä–∏–º—É—î–º–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ bbox —Ç–∞ confidence
            x_center, y_center, width, height, confidence = pred
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ confidence threshold
            if confidence > CONFIDENCE_THRESHOLD:
                # –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–æ–º–Ω–æ–∂–∏—Ç–∏ –Ω–∞ —Ä–æ–∑–º—ñ—Ä–∏ –∫–∞–¥—Ä—É
                # –ü—Ä–∏–ø—É—Å–∫–∞—î–º–æ, —â–æ –º–æ–¥–µ–ª—å –≤—ñ–¥–¥–∞—î –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏ [0, 1]
                x_center_abs = x_center * orig_width
                y_center_abs = y_center * orig_height
                width_abs = width * orig_width
                height_abs = height * orig_height
                
                # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –∑ center format –≤ corner format
                x1 = int(x_center_abs - width_abs / 2)
                y1 = int(y_center_abs - height_abs / 2)
                x2 = int(x_center_abs + width_abs / 2)
                y2 = int(y_center_abs + height_abs / 2)
                
                # –û–±–º–µ–∂—É—î–º–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∏
                x1 = max(0, min(x1, orig_width - 1))
                y1 = max(0, min(y1, orig_height - 1))
                x2 = max(0, min(x2, orig_width - 1))
                y2 = max(0, min(y2, orig_height - 1))
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –±–æ–∫—Å –º–∞—î —Ä–æ–∑—É–º–Ω–∏–π —Ä–æ–∑–º—ñ—Ä
                if x2 > x1 and y2 > y1:
                    detections.append([x1, y1, x2, y2, confidence])
        
        return detections
    
    def apply_nms(self, detections, iou_threshold):
        """–ó–∞—Å—Ç–æ—Å–æ–≤—É—î Non-Maximum Suppression"""
        if not detections:
            return []
        
        # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ confidence
        detections = sorted(detections, key=lambda x: x[4], reverse=True)
        
        # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ NMS
        keep = []
        while detections:
            # –ë–µ—Ä–µ–º–æ –¥–µ—Ç–µ–∫—Ü—ñ—é –∑ –Ω–∞–π–≤–∏—â–∏–º confidence
            current = detections.pop(0)
            keep.append(current)
            
            # –í–∏–¥–∞–ª—è—î–º–æ –¥–µ—Ç–µ–∫—Ü—ñ—ó –∑ –≤–∏—Å–æ–∫–∏–º IoU
            remaining = []
            for detection in detections:
                if self.calculate_iou(current, detection) < iou_threshold:
                    remaining.append(detection)
            detections = remaining
        
        return keep
    
    def calculate_iou(self, box1, box2):
        """–û–±—á–∏—Å–ª—é—î Intersection over Union"""
        x1_1, y1_1, x2_1, y2_1 = box1[:4]
        x1_2, y1_2, x2_2, y2_2 = box2[:4]
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –ø–ª–æ—â—É –ø–µ—Ä–µ—Ç–∏–Ω—É
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –ø–ª–æ—â—É –æ–±'—î–¥–Ω–∞–Ω–Ω—è
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def detect(self, frame):
        """–í–∏–∫–æ–Ω–∞–Ω–Ω—è –¥–µ—Ç–µ–∫—Ü—ñ—ó –æ–±'—î–∫—Ç—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é TFLite –º–æ–¥–µ–ª—ñ"""
        try:
            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥ –∫–∞–¥—Ä—É
            input_data = self.preprocess_frame(frame)
            
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –≤—Ö—ñ–¥–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä
            self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
            
            # –í–∏–∫–æ–Ω—É—î–º–æ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å
            self.interpreter.invoke()
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
            
            # –ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥
            detections = self.postprocess_output(output_data, frame.shape)
            
            # –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ NMS
            detections = self.apply_nms(detections, IOU_THRESHOLD)
            
            return detections
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ TFLite –¥–µ—Ç–µ–∫—Ü—ñ—ó: {e}")
            return []

# ----------------------
# Utility functions (–∫–æ–ø—ñ—é—î–º–æ –∑ original main.py)
# ----------------------
def resize_frame(frame, width=STANDARD_WIDTH, height=STANDARD_HEIGHT):
    """Resize frame to standard size."""
    return cv.resize(frame, (width, height))

def convert_to_gray(frame):
    """Convert frame to grayscale (3 channels)."""
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    return cv.merge([gray, gray, gray])

def draw_hints(frame, is_gray_mode, width, height, fps=0, detections_count=0):
    """Draw on-screen hints and controls with high-contrast background and smaller font."""
    color_bg = (0, 0, 0)  # Black background
    color_text = (255, 255, 255)  # White text
    font = cv.FONT_HERSHEY_SIMPLEX
    font_scale = 0.45  # Smaller font size
    thickness = 1
    y = 30
    hints = [
        (
            "Gray mode ON" if is_gray_mode else "Gray mode OFF",
            width - 250 if is_gray_mode else width - 200,
            y,
        ),
        ("Press 'ESC' to exit", 10, y + 20),
        ("Press 'r' to reset selection", 10, y + 40),
        ("Press 'g' to toggle gray mode", 10, y + 60),
        ("Press 'c' to switch to camera", 10, y + 80),
        ("Press '1' to switch to Raspberry Pi camera", 10, y + 100),
        ("Press 'v' to switch to video", 10, y + 120),
        ("Press 'n' for next video", 10, y + 140),
        ("Press 'p' for previous video", 10, y + 160),
        ("TFLite Model Active", 10, y + 180),  # –î–æ–¥–∞—î–º–æ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä TFLite
        (f"FPS: {fps:.1f}", 10, y + 200),  # FPS
        (f"Detections: {detections_count}", 10, y + 220),  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü—ñ–π
    ]
    for text, x, y_pos in hints:
        (text_width, text_height), baseline = cv.getTextSize(
            text, font, font_scale, thickness
        )
        # Draw background rectangle for text
        cv.rectangle(
            frame,
            (x - 2, y_pos - text_height - 2),
            (x + text_width + 2, y_pos + baseline + 2),
            color_bg,
            -1,
        )
        # Draw the text itself
        cv.putText(
            frame, text, (x, y_pos), font, font_scale, color_text, thickness, cv.LINE_AA
        )
    return frame

def limit_fps(frame_start_time, max_fps=30):
    """Sleep to limit the FPS to max_fps. Returns new frame_start_time."""
    frame_end_time = time.time()
    elapsed_time = frame_end_time - frame_start_time
    target_time_per_frame = 1.0 / max_fps
    if elapsed_time < target_time_per_frame:
        time.sleep(target_time_per_frame - elapsed_time)
    return time.time()

def draw_detection(frame, x1, y1, x2, y2, conf):
    """Draw detection bounding box, center, and label above the box with good readability."""
    color_box = (0, 255, 0)  # Green box
    color_center = (0, 0, 255)  # Red center dot
    color_text = (255, 255, 255)  # White text
    color_bg = (0, 0, 0)  # Black background for text
    box_width = x2 - x1
    box_height = y2 - y1
    shrink_factor = 0.7
    new_width = int(box_width * shrink_factor)
    new_height = int(box_height * shrink_factor)
    x_center = x1 + box_width // 2
    y_center = y1 + box_height // 2
    x1_new = x_center - new_width // 2
    y1_new = y_center - new_height // 2
    x2_new = x1_new + new_width
    y2_new = y1_new + new_height
    cv.rectangle(frame, (x1_new, y1_new), (x2_new, y2_new), color_box, 2)
    cv.circle(frame, (x_center, y_center), 5, color_center, -1)
    
    # Label text
    label = f"Object {conf:.2f}"
    (text_width, text_height), baseline = cv.getTextSize(
        label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1
    )
    label_y = y1_new - 10
    if label_y < text_height:
        label_y = y1_new + text_height + 10
    cv.rectangle(
        frame,
        (x1_new, label_y - text_height - 5),
        (x1_new + text_width, label_y + baseline),
        color_bg,
        -1,
    )
    cv.putText(
        frame, label, (x1_new, label_y), cv.FONT_HERSHEY_SIMPLEX, 0.5, color_text, 1
    )

# ----------------------
# Main function
# ----------------------
def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –ø—Ä–æ–≥—Ä–∞–º–∏ –∑ TFLite –º–æ–¥–µ–ª–ª—é"""
    print("üöÄ –ó–∞–ø—É—Å–∫ TFLite YOLO + DeepSort —Å–∏—Å—Ç–µ–º–∏...")
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è TFLite –º–æ–¥–µ–ª—ñ
    try:
        yolo_model = TFLiteYOLO(TFLITE_MODEL_PATH)
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è TFLite –º–æ–¥–µ–ª—ñ: {e}")
        return
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è DeepSort
    deep_sort = DeepSort(max_age=30, n_init=3)
    
    # ========================================
    # VIDEO/CAMERA SWITCHING FUNCTIONALITY
    # ========================================
    # –¶–µ–π –±–ª–æ–∫ –º–æ–∂–Ω–∞ –ª–µ–≥–∫–æ –≤–∏–¥–∞–ª–∏—Ç–∏, —è–∫—â–æ –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω
    
    cap = None
    current_video_index = 0
    is_camera_mode = True
    
    def open_camera():
        """–í—ñ–¥–∫—Ä–∏–≤–∞—î –∫–∞–º–µ—Ä—É"""
        cap = cv.VideoCapture(0)
        if cap.isOpened():
            cap.set(cv.CAP_PROP_FRAME_WIDTH, STANDARD_WIDTH)
            cap.set(cv.CAP_PROP_FRAME_HEIGHT, STANDARD_HEIGHT)
            cap.set(cv.CAP_PROP_FPS, MAX_FPS)
            print("‚úÖ –ö–∞–º–µ—Ä–∞ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")
            return cap, True
        else:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –∫–∞–º–µ—Ä—É")
            return None, False
    
    def open_video(video_index):
        """–í—ñ–¥–∫—Ä–∏–≤–∞—î –≤—ñ–¥–µ–æ —Ñ–∞–π–ª"""
        if video_index < len(VIDEO_FILES):
            video_path = VIDEO_FILES[video_index]
            if os.path.exists(video_path):
                cap = cv.VideoCapture(video_path)
                if cap.isOpened():
                    print(f"‚úÖ –í—ñ–¥–µ–æ –≤—ñ–¥–∫—Ä–∏—Ç–æ: {video_path}")
                    return cap, False
                else:
                    print(f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –≤—ñ–¥–µ–æ: {video_path}")
            else:
                print(f"‚ùå –í—ñ–¥–µ–æ —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {video_path}")
        return None, True
    
    def switch_to_next_video():
        """–ü–µ—Ä–µ–∫–ª—é—á–∞—î –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–µ –≤—ñ–¥–µ–æ"""
        nonlocal current_video_index, cap, is_camera_mode
        if not is_camera_mode:
            current_video_index = (current_video_index + 1) % len(VIDEO_FILES)
            cap.release()
            cap, is_camera_mode = open_video(current_video_index)
            return cap is not None
        return False
    
    def switch_to_previous_video():
        """–ü–µ—Ä–µ–∫–ª—é—á–∞—î –Ω–∞ –ø–æ–ø–µ—Ä–µ–¥–Ω—î –≤—ñ–¥–µ–æ"""
        nonlocal current_video_index, cap, is_camera_mode
        if not is_camera_mode:
            current_video_index = (current_video_index - 1) % len(VIDEO_FILES)
            cap.release()
            cap, is_camera_mode = open_video(current_video_index)
            return cap is not None
        return False
    
    def switch_to_camera():
        """–ü–µ—Ä–µ–∫–ª—é—á–∞—î –Ω–∞ –∫–∞–º–µ—Ä—É"""
        nonlocal cap, is_camera_mode
        cap.release()
        cap, is_camera_mode = open_camera()
        return cap is not None
    
    def switch_to_video():
        """–ü–µ—Ä–µ–∫–ª—é—á–∞—î –Ω–∞ –≤—ñ–¥–µ–æ"""
        nonlocal cap, is_camera_mode
        cap.release()
        cap, is_camera_mode = open_video(current_video_index)
        return cap is not None
    
    # ========================================
    # END OF VIDEO/CAMERA SWITCHING
    # ========================================
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–æ—á–∞—Ç–∫–æ–≤–æ–≥–æ –¥–∂–µ—Ä–µ–ª–∞
    cap, is_camera_mode = open_camera()
    
    if cap is None:
        # –Ø–∫—â–æ –∫–∞–º–µ—Ä–∞ –Ω–µ –ø—Ä–∞—Ü—é—î, –ø—Ä–æ–±—É—î–º–æ –≤—ñ–¥–µ–æ
        cap, is_camera_mode = open_video(current_video_index)
        if cap is None:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –Ω—ñ –∫–∞–º–µ—Ä—É, –Ω—ñ –≤—ñ–¥–µ–æ")
            return
    
    print("üìã –ö–µ—Ä—É–≤–∞–Ω–Ω—è:")
    print("   ESC - –≤–∏—Ö—ñ–¥")
    print("   g - –ø–µ—Ä–µ–º–∏–∫–∞–Ω–Ω—è –≤ —á–æ—Ä–Ω–æ-–±—ñ–ª–∏–π —Ä–µ–∂–∏–º")
    print("   r - —Å–∫–∏–¥–∞–Ω–Ω—è —Ç—Ä–µ–∫–µ—Ä–∞")
    print("   c - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è –Ω–∞ –∫–∞–º–µ—Ä—É")
    print("   1 - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è –Ω–∞ Raspberry Pi –∫–∞–º–µ—Ä—É")
    print("   v - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è –Ω–∞ –≤—ñ–¥–µ–æ")
    print("   n - –Ω–∞—Å—Ç—É–ø–Ω–µ –≤—ñ–¥–µ–æ")
    print("   p - –ø–æ–ø–µ—Ä–µ–¥–Ω—î –≤—ñ–¥–µ–æ")
    
    # –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª
    frame_count = 0
    is_gray_mode = False
    fps_counter = 0
    fps_start_time = time.time()
    fps_display = 0
    
    while True:
        frame_start_time = time.time()
        
        # –ó—á–∏—Ç—É—î–º–æ –∫–∞–¥—Ä
        ret, frame = cap.read()
        if not ret:
            if not is_camera_mode:
                # –Ø–∫—â–æ –≤—ñ–¥–µ–æ –∑–∞–∫—ñ–Ω—á–∏–ª–æ—Å—è, –ø–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ
                if not switch_to_next_video():
                    print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–µ –≤—ñ–¥–µ–æ")
                    break
                continue
            else:
                print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑—á–∏—Ç–∞—Ç–∏ –∫–∞–¥—Ä –∑ –∫–∞–º–µ—Ä–∏")
                break
        
        # –ó–º—ñ–Ω—é—î–º–æ —Ä–æ–∑–º—ñ—Ä –∫–∞–¥—Ä—É
        frame = resize_frame(frame)
        
        # –ß–æ—Ä–Ω–æ-–±—ñ–ª–∏–π —Ä–µ–∂–∏–º
        if is_gray_mode:
            frame = convert_to_gray(frame)
        
        # –î–µ—Ç–µ–∫—Ü—ñ—è –æ–±'—î–∫—Ç—ñ–≤ (–∫–æ–∂–µ–Ω YOLO_SKIP_FRAMES –∫–∞–¥—Ä –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
        detections = []
        if frame_count % YOLO_SKIP_FRAMES == 0:
            try:
                detections = yolo_model.detect(frame)
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –¥–µ—Ç–µ–∫—Ü—ñ—ó: {e}")
        
        # –ú–∞–ª—é—î–º–æ –¥–µ—Ç–µ–∫—Ü—ñ—ó
        for detection in detections:
            x1, y1, x2, y2, conf = detection
            if conf > CONFIDENCE_THRESHOLD:
                draw_detection(frame, int(x1), int(y1), int(x2), int(y2), conf)
        
        # –û–±—á–∏—Å–ª—é—î–º–æ FPS
        fps_counter += 1
        if time.time() - fps_start_time >= 1.0:
            fps_display = fps_counter
            fps_counter = 0
            fps_start_time = time.time()
        
        # –ü—ñ–¥–∫–∞–∑–∫–∏
        frame = draw_hints(frame, is_gray_mode, STANDARD_WIDTH, STANDARD_HEIGHT, 
                          fps_display, len(detections))
        
        # –ü–æ–∫–∞–∑—É—î–º–æ –∫–∞–¥—Ä
        cv.imshow("TFLite YOLO + DeepSort", frame)
        
        # –û–±—Ä–æ–±–∫–∞ –∫–ª–∞–≤—ñ—à
        key = cv.waitKey(1) & 0xFF
        if key == 27:  # ESC
            break
        elif key == ord('g'):
            is_gray_mode = not is_gray_mode
            print(f"üé® –ß–æ—Ä–Ω–æ-–±—ñ–ª–∏–π —Ä–µ–∂–∏–º: {'ON' if is_gray_mode else 'OFF'}")
        elif key == ord('r'):
            deep_sort = DeepSort(max_age=30, n_init=3)
            print("üîÑ –¢—Ä–µ–∫–µ—Ä —Å–∫–∏–Ω—É—Ç–æ")
        elif key == ord('c'):
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è –Ω–∞ –∫–∞–º–µ—Ä—É
            if not switch_to_camera():
                print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–∏—Å—è –Ω–∞ –∫–∞–º–µ—Ä—É")
                break
        elif key == ord('1'):  # Switch to Raspberry Pi camera (GStreamer)
            if cap:
                cap.release()
            cap = cv.VideoCapture(
                "v4l2src device=/dev/video0 ! videoconvert ! appsink", cv.CAP_GSTREAMER
            )
            is_camera_mode = True
            print("üîÑ –ü–µ—Ä–µ–º–∏–∫–∞–Ω–Ω—è –Ω–∞ Raspberry Pi –∫–∞–º–µ—Ä—É (GStreamer)")
            if not cap.isOpened():
                print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ Raspberry Pi –∫–∞–º–µ—Ä—É")
                # Fallback to regular camera
                cap, is_camera_mode = open_camera()
                if cap is None:
                    print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –∑–≤–∏—á–∞–π–Ω—É –∫–∞–º–µ—Ä—É")
                    break
        elif key == ord('v'):
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è –Ω–∞ –≤—ñ–¥–µ–æ
            if not switch_to_video():
                print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–∏—Å—è –Ω–∞ –≤—ñ–¥–µ–æ")
                break
        elif key == ord('n'):
            # –ù–∞—Å—Ç—É–ø–Ω–µ –≤—ñ–¥–µ–æ
            if not switch_to_next_video():
                print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–µ –≤—ñ–¥–µ–æ")
                break
        elif key == ord('p'):
            # –ü–æ–ø–µ—Ä–µ–¥–Ω—î –≤—ñ–¥–µ–æ
            if not switch_to_previous_video():
                print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–∫—Ä–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—î –≤—ñ–¥–µ–æ")
                break
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è FPS (–∑–±—ñ–ª—å—à—É—î–º–æ –¥–æ 60 –¥–ª—è –∫—Ä–∞—â–æ—ó –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
        frame_start_time = limit_fps(frame_start_time, 60)
        frame_count += 1
    
    # Cleanup
    if cap:
        cap.release()
    cv.destroyAllWindows()
    print("üëã –ü—Ä–æ–≥—Ä–∞–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

if __name__ == "__main__":
    main()
